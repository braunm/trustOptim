<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Using trustOptim for Unconstrained Nonlinear Optimization with Sparse Hessians • trustOptim</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/spacelab/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Using trustOptim for Unconstrained Nonlinear Optimization with Sparse Hessians">
<meta property="og:description" content="trustOptim">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=G-XVJF7FJCCT"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-XVJF7FJCCT');
</script>
</head>
<body data-spy="scroll" data-target="#toc">
    

    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">trustOptim</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.8.7.2</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/trustOptim-demo.html">Using trustOptim for Unconstrained Nonlinear Optimization with Sparse Hessians</a>
    </li>
    <li>
      <a href="../articles/trustOptim-quick.html">A Quick Demo of trustOptim</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/braunm/trustOptim/" class="external-link">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><script src="trustOptim-demo_files/header-attrs-2.11/header-attrs.js"></script><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Using trustOptim for Unconstrained Nonlinear Optimization with Sparse Hessians</h1>
                        <h4 data-toc-skip class="author">Michael Braun</h4>
            
            <h4 data-toc-skip class="date">2021-09-23</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/braunm/trustOptim/blob/master/vignettes/trustOptim-demo.Rmd" class="external-link"><code>vignettes/trustOptim-demo.Rmd</code></a></small>
      <div class="hidden name"><code>trustOptim-demo.Rmd</code></div>

    </div>

    
    
<p>Much of this vignette was originally published as <span class="citation">Braun (2014)</span>. Please cite that article when using this package in your own research. This version of the vignette is abridged with respect to the underlying theory, and the comparison with other methods. It has more of a focus on how to use the package.</p>
<div id="why-use-trustoptim" class="section level1">
<h1 class="hasAnchor">
<a href="#why-use-trustoptim" class="anchor" aria-hidden="true"></a>Why use trustOptim?</h1>
<p>The need to optimize continuous nonlinear functions occurs frequently in statistics, most notably in maximum likelihood and <em>maximum a posteriori</em> (MAP) estimation. Users of <strong>R</strong> have a choice of dozens of optimization algorithms. The most readily available algorithms are those that are accessed from the <code>optim</code> function in the base <em>R</em> distribution, and from the many contributed packages that are described in the CRAN Task View for <em>Optimization and Mathematical Programming</em> <span class="citation">(Theussel 2013)</span>. Any particular algorithm may be more appropriate for some problems than for others, and having such a large number of alternatives allows the informed <strong>R</strong> user to choose the best tool for the task at hand.</p>
<p>One limitation of most of these algorithms is that they can be difficult to use when there is a large number of decision variables. Search methods can be inefficient with a massive number of parameters because the search space is large, and they do not exploit information about slope and curvature to speed up the time to convergence. Conjugate gradient and quasi-Newton methods trace the curvature of the function by using successive gradients to approximate the inverse Hessian. However, if the algorithm stores the entire dense inverse Hessian, its use is resource-intensive when the number of parameters is large. For example, the Hessian for a 50,000 parameter model requires 20GB of RAM to store it as a standard, dense base <strong>R</strong> matrix.</p>
<p>The <em>trustOptim</em> package provides a trust region algorithm that is optimized for problems for which the Hessian is sparse. Sparse Hessians occur when a large number of the cross-partial derivatives of the objective function are zero. For example, suppose we want to find the mode of a log posterior density for a Bayesian hierarchical model. If we assume that individual-level parameter vectors <span class="math inline">\(\beta_i\)</span> and <span class="math inline">\(\beta_j\)</span> are conditionally independent, the cross-partial derivatives between all elements of <span class="math inline">\(\beta_i\)</span> and <span class="math inline">\(\beta_j\)</span> are zero. If the model includes a large number of heterogeneous units, and a relatively small number of population-level parameters, the proportion of non-zero entries in the Hessian will be small. Since we know up front which elements of the Hessian are non-zero, we need to compute, store, and operate on only those non-zero elements. By storing sparse Hessians in a compressed format, and using a library of numerical algorithms that are efficient for sparse matrices, we can run the optimization algorithms faster, with a smaller memory footprint, than algorithms that operate on dense Hessians.</p>
<p>The details of the trust region algorithm are included at the end of this vignette. The vignette for the <em>sparseHessianFD</em> package includes a more detailed discussion of Hessian sparsity patterns.</p>
</div>
<div id="example-function" class="section level1">
<h1 class="hasAnchor">
<a href="#example-function" class="anchor" aria-hidden="true"></a>Example function</h1>
<p>Before going into the details of how to use the package, let’s consider the following example of an objective function with a sparse Hessian. Suppose we have a dataset of <span class="math inline">\(N\)</span> households, each with <span class="math inline">\(T\)</span> opportunities to purchase a particular product. Let <span class="math inline">\(y_i\)</span> be the number of times household <span class="math inline">\(i\)</span> purchases the product, out of the <span class="math inline">\(T\)</span> purchase opportunities. Furthermore, let <span class="math inline">\(p_i\)</span> be the probability of purchase; <span class="math inline">\(p_i\)</span> is the same for all <span class="math inline">\(T\)</span> opportunities, so we can treat <span class="math inline">\(y_i\)</span> as a binomial random variable. The purchase probability <span class="math inline">\(p_i\)</span> is heterogeneous, and depends on both <span class="math inline">\(k\)</span> continuous covariates <span class="math inline">\(x_i\)</span>, and a heterogeneous coefficient vector <span class="math inline">\(\beta_i\)</span>, such that <span class="math display">\[
  p_i=\frac{\exp(x_i'\beta_i)}{1+\exp(x_i'\beta_i)},~i=1 ... N
\]</span></p>
<p>The coefficients can be thought of as sensitivities to the covariates, and they are distributed across the population of households following a multivariate normal distribution with mean <span class="math inline">\(\mu\)</span> and covariance <span class="math inline">\(\Sigma\)</span>. We assume that we know <span class="math inline">\(\Sigma\)</span>, but we do not know <span class="math inline">\(\mu\)</span>. Instead, we place a multivariate normal prior on <span class="math inline">\(\mu\)</span>, with mean <span class="math inline">\(0\)</span> and covariance <span class="math inline">\(\Omega_0\)</span>. Thus, each <span class="math inline">\(\beta_i\)</span>, and <span class="math inline">\(\mu\)</span> are <span class="math inline">\(k-\)</span>dimensional vectors, and the total number of unknown variables in the model is <span class="math inline">\((N+1)k\)</span>.</p>
<p>The log posterior density, ignoring any normalization constants, is <span class="math display">\[
  \log \pi(\beta_{1:N},\mu|Y, X, \Sigma_0,\Omega_0)=\sum_{i=1}^Np_i^{y_i}(1-p_i)^{T-y_i}
  -\frac{1}{2}\left(\beta_i-\mu\right)'\Sigma^{-1}\left(\beta_i-\mu\right)
-\frac{1}{2}\mu'\Omega_0^{-1}\mu
\]</span></p>
<p>Since the <span class="math inline">\(\beta_i\)</span> are drawn iid from a multivariate normal, <span class="math inline">\(\dfrac{\partial^2\log\pi }{\partial\beta_i\beta_j}=0\)</span> for all <span class="math inline">\(i\neq j\)</span>. We also know that all of the <span class="math inline">\(\beta_i\)</span> are correlated with <span class="math inline">\(\mu\)</span>. The structure of the Hessian depends on how the variables are ordered within the vector. One such ordering is to group all of the coefficients for each unit together.</p>
<p><span class="math display">\[
\beta_{11},...,\beta_{1k},\beta_{21},...,\beta_{2k},...~,~...~,~\beta_{N1}~,~...~,~\beta_{Nk},\mu_1,...,\mu_k
\]</span></p>
<p>In this case, the Hessian has a “block-arrow” structure. For example, if <span class="math inline">\(N=6\)</span> and <span class="math inline">\(k=2\)</span>, then there are 14 total variables, and the Hessian will have the following pattern.</p>
<pre><code># 14 x 14 sparse Matrix of class "lgCMatrix"
#                                  
#  [1,] | | . . . . . . . . . . | |
#  [2,] | | . . . . . . . . . . | |
#  [3,] . . | | . . . . . . . . | |
#  [4,] . . | | . . . . . . . . | |
#  [5,] . . . . | | . . . . . . | |
#  [6,] . . . . | | . . . . . . | |
#  [7,] . . . . . . | | . . . . | |
#  [8,] . . . . . . | | . . . . | |
#  [9,] . . . . . . . . | | . . | |
# [10,] . . . . . . . . | | . . | |
# [11,] . . . . . . . . . . | | | |
# [12,] . . . . . . . . . . | | | |
# [13,] | | | | | | | | | | | | | |
# [14,] | | | | | | | | | | | | | |</code></pre>
<p>There are 196 elements in this symmetric matrix, but only 76 are non-zero, and only 45 values are unique. Although the reduction in RAM from using a sparse matrix structure for the Hessian may be modest, consider what would happen if <span class="math inline">\(N=1000\)</span> instead. In that case, there are 2002 variables in the problem, and more than <span class="math inline">\(4\)</span> million elements in the Hessian. However, only <span class="math inline">\(12004\)</span> of those elements are non-zero. If we work with only the lower triangle of the Hessian we only need to work with only 7003 values.</p>
</div>
<div id="using-the-package" class="section level1">
<h1 class="hasAnchor">
<a href="#using-the-package" class="anchor" aria-hidden="true"></a>Using the package</h1>
<p>The functions for computing the objective function, gradient and Hessian for this example are in the R/binary.R file. The package also includes a sample dataset with simulated data from the binary choice example. This dataset can be access with the <code>data(binary)</code> call.</p>
<p>To start, we load the data, set some dimension parameters, set prior values for <span class="math inline">\(\Sigma^{-1}\)</span> and <span class="math inline">\(\Omega^{-1}\)</span>, and simulate a vector of variables at which to evaluate the function.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="va">binary</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/utils/str.html" class="external-link">str</a></span><span class="op">(</span><span class="va">binary</span><span class="op">)</span></code></pre></div>
<pre><code># List of 3
#  $ Y: int [1:200] 16 1 30 70 51 52 0 27 59 15 ...
#  $ X: num [1:2, 1:200] 1.5587 0.0705 0.1293 1.7151 0.4609 ...
#  $ T: num 100</code></pre>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">N</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">binary</span><span class="op">$</span><span class="va">Y</span><span class="op">)</span>
<span class="va">k</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html" class="external-link">NROW</a></span><span class="op">(</span><span class="va">binary</span><span class="op">$</span><span class="va">X</span><span class="op">)</span>
<span class="va">nvars</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/integer.html" class="external-link">as.integer</a></span><span class="op">(</span><span class="va">N</span><span class="op">*</span><span class="va">k</span> <span class="op">+</span> <span class="va">k</span><span class="op">)</span>
<span class="va">start</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="va">nvars</span><span class="op">)</span> <span class="co">## random starting values</span>
<span class="va">priors</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>inv.Sigma <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/rWishart.html" class="external-link">rWishart</a></span><span class="op">(</span><span class="fl">1</span>,<span class="va">k</span><span class="op">+</span><span class="fl">5</span>,<span class="fu"><a href="https://rdrr.io/r/base/diag.html" class="external-link">diag</a></span><span class="op">(</span><span class="va">k</span><span class="op">)</span><span class="op">)</span><span class="op">[</span>,,<span class="fl">1</span><span class="op">]</span>,
               inv.Omega <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/diag.html" class="external-link">diag</a></span><span class="op">(</span><span class="va">k</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<p>This dataset represents the simulated choices for <span class="math inline">\(N= 200\)</span> customers over <span class="math inline">\(T= 100\)</span> purchase opportunties, where the probability of purchase is influenced by <span class="math inline">\(k= 2\)</span> covariates.</p>
<p>The objective function for the binary choice example is <code>binary.f</code>, the gradient function is <code>binary.grad</code>, and the Hessian function is <code>binary.hess</code>. The first argument to both is the variable vector, and the argument lists must be the same for both. For this example, we need to provide the data list “binary” (<span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span> and <span class="math inline">\(T\)</span>) and the prior parameter list (<span class="math inline">\(\Sigma^{-1}\)</span> and <span class="math inline">\(\Omega^{-1}\)</span>). The <code>binary.hess</code> function returns the Hessian as a <code>dgCMatrix</code> object, which is a compressed sparse matrix class defined in the Matrix package.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">opt</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/trust.optim.html">trust.optim</a></span><span class="op">(</span><span class="va">start</span>, fn<span class="op">=</span><span class="va">binary.f</span>,
                   gr <span class="op">=</span> <span class="va">binary.grad</span>,
                   hs <span class="op">=</span> <span class="va">binary.hess</span>,
                   method <span class="op">=</span> <span class="st">"Sparse"</span>,
                   control <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>
                       start.trust.radius<span class="op">=</span><span class="fl">5</span>,
                       stop.trust.radius <span class="op">=</span> <span class="fl">1e-7</span>,
                       prec<span class="op">=</span><span class="fl">1e-7</span>,
                       report.precision<span class="op">=</span><span class="fl">1L</span>,
                       maxit<span class="op">=</span><span class="fl">500L</span>,
                       preconditioner<span class="op">=</span><span class="fl">1L</span>,
                       function.scale.factor<span class="op">=</span><span class="op">-</span><span class="fl">1</span>
                   <span class="op">)</span>,
                   data<span class="op">=</span><span class="va">binary</span>, priors<span class="op">=</span><span class="va">priors</span>
                   <span class="op">)</span></code></pre></div>
<pre><code># Beginning optimization
# 
# iter       f     nrm_gr                     status
#   1   15977.1    769.5     Continuing - TR expand
#   2   12359.7    278.2                 Continuing
#   3   12359.7    278.2   Continuing - TR contract
#   4   12059.7    261.9                 Continuing
#   5   12059.7    261.9   Continuing - TR contract
#   6   11670.0    310.8                 Continuing
#   7   11670.0    310.8   Continuing - TR contract
#   8   11440.0     67.5     Continuing - TR expand
#   9   11304.5      7.2     Continuing - TR expand
#  10   11303.8      1.0                 Continuing
#  11   11303.8      0.0                 Continuing
#  12   11303.8      0.0                 Continuing
#  13   11303.8      0.0                 Continuing
# 
# Iteration has terminated
#  13   11303.8      0.0                    Success</code></pre>
<div id="control-parameters" class="section level2">
<h2 class="hasAnchor">
<a href="#control-parameters" class="anchor" aria-hidden="true"></a>Control parameters</h2>
<p>The <code>control</code> argument takes a list of options, all of which are described in the package manual. Most of these arguments are related to the internal workings of the trust region algorithm, such as how close a step needs to be to the border of the trust region before the region expands. However, there are a few arguments that deserve some special attention.</p>
<div id="scaling-the-objective-function" class="section level3">
<h3 class="hasAnchor">
<a href="#scaling-the-objective-function" class="anchor" aria-hidden="true"></a>Scaling the objective function</h3>
<p>The algorithms in the package <em>minimize</em> the objective function by default. When the <code>function.scale.factor</code> option is specified, the objective function, gradient and Hessian are all multiplied by that value throughout the optimization procedure. If <code>function.scale.factor=-1</code>, then then <code>trust.optim</code> will maximize the objective function.</p>
</div>
<div id="stopping-criteria" class="section level3">
<h3 class="hasAnchor">
<a href="#stopping-criteria" class="anchor" aria-hidden="true"></a>Stopping criteria</h3>
<p>The <code>trust.optim</code> function will stop when the Euclidean norm of the gradient is less that <code>sqrt(nvars) * prec</code>, where <code>nvars</code> is the length of the parameter vector, and <code>prec</code> is specified in the control list (the default is <code>sqrt(.Machine\$double.eps)</code>, which is the square root of the computer’s floating point precision. However, sometimes the algorithm cannot get the gradient to be that flat. When that occurs, the trust region will shrink, until its radius is less than the value of the <code>stop.trust.radius</code> parameter. The algorithm will then stop with the message “Radius of trust region is less than <code>stop.trust.radius</code>.” This event is not necessarily a problem if the norm of the gradient is still small enough that the gradient is flat for all practical purposes. For example, suppose we set <code>prec</code> to be <span class="math inline">\(10^{-7}\)</span> and that, for numerical reasons, the norm of the gradient simply cannot get below <span class="math inline">\(10^{-6}\)</span>. If the norm of the gradient were the only stopping criterion, the algorithm would continue to run, even though it is probably close enough to the local optimum. With this alternative stopping criterion, the algorithm will also stop when it is clear that the algorithm can no longer take a step that leads to an improvement in the objective function.</p>
<p>There is, of course, a third stopping criterion. The <code>maxit</code> argument is the maximum number of iterations the algorithm should run before stopping. However, keep in mind that if the algorithm stops at <code>maxit</code>, it is almost certainly not at a local optimum.</p>
<blockquote>
<blockquote>
<p>Note that many other nonlinear optimizers, including <code>optim</code>, do not use the norm of the gradient as a stopping criterion. Instead, they stop when the absolute or relative changes in the objective function are less than some tolerance value. This often causes those optimizers to stop prematurely, when the estimates of the gradient and/or Hessian are not precise, or if there are some regions of the domain where the objective function is nearly flat. In theory, this should never happen, but in reality, it happens <em>all the time</em>. For an unconstrained optimization problem, there is no reason why the norm of the gradient should not be zero (within numerical precision) before the algorithm stops.</p>
</blockquote>
</blockquote>
</div>
</div>
<div id="preconditioners" class="section level2">
<h2 class="hasAnchor">
<a href="#preconditioners" class="anchor" aria-hidden="true"></a>Preconditioners</h2>
<p>Currently, the package offers two preconditioners: an identity preconditioner (no preconditioning), and an inexact modified Cholesky preconditioner, as in Algorithm 7.3 of <span class="citation">Nocedal and Wright (2006)</span>. The identity and diagonal preconditioners are available for all of the methods. For the <em>Sparse</em> method, the modified Cholesky preconditioner will use a positive definite matrix that is close to the potentially indefinite Hessian (<code>trust.optim</code> does <em>not</em> require that the objective function be positive definite). For <em>BFGS</em>, the modified Cholesky preconditioner is available because <em>BFGS</em> updates are always positive definite. If the user selects a modified Cholesky preconditioner for <em>SR1</em>, the algorithm will use the identity preconditioner instead.</p>
<p>There is no general rule for selecting preconditioners. There will be a tradeoff between the number of iterations needs to solve the problem and the time it takes to compute any particular preconditioner. In some cases, the identity preconditioner may even solve the problem in fewer iterations than a modified Cholesky preconditioner.</p>
</div>
<div id="result-object" class="section level2">
<h2 class="hasAnchor">
<a href="#result-object" class="anchor" aria-hidden="true"></a>Result object</h2>
<p>The call ot <code>trust.optim</code> returns a list of values.</p>
<ul>
<li>
<strong>fval</strong>: the value of the objective function at the optimum</li>
<li>
<strong>solution</strong>: the optimum</li>
<li>
<strong>gradient</strong>: the gradient of the objective function at the optimum (all elements should be very close to zero)</li>
<li>
<strong>hessian</strong>: the Hessian of the objective function at the optimum, as an object of class <em>dsCMatrix</em>.</li>
<li>
<strong>iterations</strong>: number of iterations</li>
<li>
<strong>status</strong>: A status message (should be “Success”), or possibly a note that the trust region radius is less than <code>stop.trust.region</code>.</li>
<li>
<strong>trust.radius</strong>: trust region radius when the algorithm stopped.</li>
<li>
<strong>nnz</strong>: number of nonzero elements in the lower triangle of the Hessian</li>
<li>
<strong>method</strong>: the optimization method that was used (Sparse, SR1 or BFGS).</li>
<li>
<strong>nnz</strong>: for the Sparse method only, the number of nonzero elements in the Hessian.</li>
</ul>
<p>See the package manual for more details.</p>
</div>
</div>
<div id="algorithmic-details" class="section level1">
<h1 class="hasAnchor">
<a href="#algorithmic-details" class="anchor" aria-hidden="true"></a>Algorithmic details</h1>
<p>Consider <span class="math inline">\(f(x)\)</span>, an objective function over a <span class="math inline">\(P\)</span>-dimensional vector that we want to minimize. Let <span class="math inline">\(g\)</span> be the gradient, and let <span class="math inline">\(B\)</span> be the Hessian. The goal is to find a local minimum of <span class="math inline">\(f(x)\)</span>, with no constraints on <span class="math inline">\(x\)</span>, within some window of numerical precision (i.e., where <span class="math inline">\(\|g\|_2 / \sqrt{p}&lt;\epsilon\)</span> for small <span class="math inline">\(\epsilon&gt;0\)</span>). We will assume that <span class="math inline">\(B\)</span> is positive definite at the local optimum, but not necessarily at other values of <span class="math inline">\(x\)</span>. Iterations are indexed by <span class="math inline">\(t\)</span>.</p>
<div id="trust-region-methods-for-nonlinear-optimization" class="section level2">
<h2 class="hasAnchor">
<a href="#trust-region-methods-for-nonlinear-optimization" class="anchor" aria-hidden="true"></a>Trust region methods for nonlinear optimization</h2>
<p>The details of trust region methods are described in both <span class="citation">Nocedal and Wright (2006)</span> and <span class="citation">Conn, Gould, and Toint (2000)</span>, and the following exposition borrows heavily from both sources. At each iteration of a trust region algorithm, we construct a quadratic approximation to the objective function at <span class="math inline">\(x_t\)</span>, and minimize that approximation, subject to a constraint that the solution falls within a trust region with radius <span class="math inline">\(d_t\)</span>. More formally, each iteration of the trust region algorithm involves solving the “trust region subproblem,” or TRS.</p>
<p><span class="math display">\[
\begin{align}
\tag{TRS}\label{eq:TRS}
\min_{s\in R^p} f^*(s)&amp; = f(x_t) + g_t^\top s + \frac{1}{2}s^\top B_ts\qquad\text{s.t. }\|s\|_M\leq d_t\\
s_t&amp;=\arg\min_{s\in R^p} f^*(s) \qquad\text{s.t. }\|s\|_M\leq d_t
\end{align}
\]</span></p>
<p>The norm <span class="math inline">\(\|\cdot\|_M\)</span> is a Mahalanobis norm with respect to some positive definite matrix <span class="math inline">\(M\)</span>.</p>
<p>Let <span class="math inline">\(s_t\)</span> be the solution to the  for iteration <span class="math inline">\(t\)</span>, and consider the ratio <span class="math display">\[\begin{align}
  \label{eq:2}
  \rho_t=\frac{f(x_t)-f(x_t+s_t)}{f^*(x_t)-f^*(x_t+s_t)}
\end{align}\]</span> This ratio is the improvement in the objective function that we would get from a move from <span class="math inline">\(x_t\)</span> to <span class="math inline">\(x_{t+1}\)</span>, where <span class="math inline">\(x_{t+1}=x_t+s_t\)</span>, relative to the improvement that is predicted by the quadratic approximation. Let <span class="math inline">\(\eta_1\)</span> be the minimum value of <span class="math inline">\(\rho_t\)</span> for which we deem it “worthwhile” to move from <span class="math inline">\(x_t\)</span> to <span class="math inline">\(x_{t+1}\)</span>, and let <span class="math inline">\(\eta_2\)</span> be the maximum <span class="math inline">\(\rho_t\)</span> that would trigger a shrinkage in the trust region. If <span class="math inline">\(\rho_t &lt; \eta_2\)</span>, or if <span class="math inline">\(f(x_t+s_t)\)</span> is not finite, we shrink the trust region by reducing <span class="math inline">\(d_t\)</span> by some predetermined factor, and compute a new <span class="math inline">\(s_t\)</span> by solving the  again. If <span class="math inline">\(\rho_t&gt;\eta_1\)</span>, we move to <span class="math inline">\(x_{t+1}=x_t+s_t\)</span>. Also, if we do accept the move, and <span class="math inline">\(s_t\)</span> is on the border of the trust region, we expand the trust region by increasing <span class="math inline">\(d_t\)</span>, again by some predetermined factor. The idea is to not move to a new <span class="math inline">\(x\)</span> if <span class="math inline">\(f(x_{t+1})\)</span> would be worse than <span class="math inline">\(f(x_t)\)</span>. By expanding the trust region, we can propose larger jumps, and potentially reach the optimum more quickly. We want to propose only moves that are among those that we “trust” to give reasonable values of <span class="math inline">\(f(x)\)</span>. If it turns out that a move leads to a large improvement in the objective function, and that the proposed move was constrained by the radius of the trust region, we want to expand the trust region so we can take larger steps. If the proposed move is bad, we should then reduce the size of the region we trust, and try to find another step that is closer to the current iterate. Of course, there is no reason that the trust region needs to change after a particular iteration, especially if the solution to the TRS is at an internal point.</p>
<p>There are a number of different ways to solve the TRS; <span class="citation">Conn, Gould, and Toint (2000)</span> is authoritative and encyclopedic in this area. The <em>trustOptim</em> package uses the method described in <span class="citation">Steihaug (1983)</span>. The Steihaug algorithm is, essentially, a conjugate gradient solver for a constrained quadratic program. If <span class="math inline">\(B_t\)</span> is positive definite, the Steihaug solution to the  will be exact, up to some level of numerical precision. However, if <span class="math inline">\(B_t\)</span> is indefinite, the algorithm could try to move in a direction of negative curvature. If the algorithm happens to stumble on such a direction, it goes back to the last direction that it moved, runs in that direction to the border of the trust region, and returns that point of intersection with the trust region border as the “solution” to the . This solution is not necessarily the true minimizer of the , but it still might provide sufficient improvement in the objective function such that <span class="math inline">\(\rho_t&gt;\eta_1\)</span>. If not, we shrink the trust region and try again. As an alternative to the Steihaug algorithm for solving the , <span class="citation">Conn, Gould, and Toint (2000)</span> suggest using the Lanczos algorithm. The Lanczos approach may be more likely to find a better solution to the TRS when <span class="math inline">\(B_t\)</span> is indefinite, but at some additional computational cost. We include only the Steihaug algorithm for now, because it still seems to work well, especially for sparse problems.</p>
<p>As with other conjugate gradient methods, one way to speed up the Steihaug algorithm is to rescale the trust region subproblem with a preconditioner <span class="math inline">\(M\)</span>. Note that the constraint in  is expressed as an <span class="math inline">\(M\)</span>-norm, rather than an Euclidean norm. The positive definite matrix <span class="math inline">\(M\)</span> should be close enough to the Hessian that <span class="math inline">\(M^{-1}B_t\approx I\)</span>, but still cheap enough to compute that the cost of using the preconditioner does not exceed the benefits. Of course, the ideal preconditioner would be <span class="math inline">\(B_t\)</span> itself, but <span class="math inline">\(B_t\)</span> is not necessarily positive definite, and we may not be able to estimate it fast enough for preconditioning to be worthwhile. In this case, one could use a modified Cholesky decomposition, as described in <span class="citation">Nocedal and Wright (2006)</span>.</p>
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="hasAnchor">
<a href="#references" class="anchor" aria-hidden="true"></a>References</h1>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-R_trustOptim" class="csl-entry">
Braun, Michael. 2014. <span>“<span class="nocase">t</span>rust<span>O</span>ptim: An <span>R</span> Package for Trust Region Optimization with Sparse Hessians.”</span> <em>Journal of Statistical Software</em> 60 (4): 1–16. <a href="https://www.jstatsoft.org/v60/i04/" class="external-link">https://www.jstatsoft.org/v60/i04/</a>.
</div>
<div id="ref-ConnGould2000" class="csl-entry">
Conn, Andrew R, Nicholas I M Gould, and Philippe L Toint. 2000. <em>Trust-Region Methods</em>. Philadelphia: SIAM-MPS.
</div>
<div id="ref-NocedalWright2006" class="csl-entry">
Nocedal, Jorge, and Stephen J Wright. 2006. <em>Numerical Optimization</em>. Second edition. Springer-Verlag.
</div>
<div id="ref-Steihaug1983" class="csl-entry">
Steihaug, Trond. 1983. <span>“The Conjugate Gradient Method and Trust Regions in Large Scale Optimization.”</span> <em><span>SIAM</span> Journal on Numerical Analysis</em> 20 (3): 626–37. <a href="https://doi.org/10.1137/0720042" class="external-link">https://doi.org/10.1137/0720042</a>.
</div>
<div id="ref-R_OptimTaskView" class="csl-entry">
Theussel, Stefan. 2013. <span>“<span>CRAN</span> Task View: Optimization and Mathematical Programming.”</span> 2013. <a href="https://cran.r-project.org/view=Optimization" class="external-link">https://cran.r-project.org/view=Optimization</a>.
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by <a href="https://braunm.github.io" class="external-link external-link">Michael Braun</a>.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link external-link">pkgdown</a> 1.6.1.9001.</p>
</div>

      </footer>
</div>

  


  

  </body>
</html>
