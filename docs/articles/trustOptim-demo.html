<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Using trustOptim for Unconstrained Nonlinear Optimization with Sparse Hessians • trustOptim</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.1/jquery.min.js" integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/spacelab/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Using trustOptim for Unconstrained Nonlinear Optimization with Sparse Hessians">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=G-XVJF7FJCCT"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-XVJF7FJCCT');
</script>
</head>
<body data-spy="scroll" data-target="#toc">


    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">trustOptim</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">0.8.7.4</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Articles

    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/trustOptim-demo.html">Using trustOptim for Unconstrained Nonlinear Optimization with Sparse Hessians</a>
    </li>
    <li>
      <a href="../articles/trustOptim-quick.html">A Quick Demo of trustOptim</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/braunm/trustOptim/" class="external-link">
    <span class="fab fa-github fa-lg"></span>

  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->



      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Using trustOptim for Unconstrained Nonlinear
Optimization with Sparse Hessians</h1>
                        <h4 data-toc-skip class="author">Michael
Braun</h4>
            
            <h4 data-toc-skip class="date">2026-01-12</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/braunm/trustOptim/blob/HEAD/vignettes/trustOptim-demo.Rmd" class="external-link"><code>vignettes/trustOptim-demo.Rmd</code></a></small>
      <div class="hidden name"><code>trustOptim-demo.Rmd</code></div>

    </div>

    
    
<p>Much of this vignette was originally published as <span class="citation">Braun (2014)</span>. Please cite that article when
using this package in your own research. This version of the vignette is
abridged with respect to the underlying theory, and the comparison with
other methods. It has more of a focus on how to use the package.</p>
<div class="section level2">
<h2 id="why-use-trustoptim">Why use trustOptim?<a class="anchor" aria-label="anchor" href="#why-use-trustoptim"></a>
</h2>
<p>The need to optimize continuous nonlinear functions occurs frequently
in statistics, most notably in maximum likelihood and <em>maximum a
posteriori</em> (MAP) estimation. Users of <strong>R</strong> have a
choice of dozens of optimization algorithms. The most readily available
algorithms are those that are accessed from the <code>optim</code>
function in the base <em>R</em> distribution, and from the many
contributed packages that are described in the CRAN Task View for
<em>Optimization and Mathematical Programming</em> <span class="citation">(Theussel 2013)</span>. Any particular algorithm may be
more appropriate for some problems than for others, and having such a
large number of alternatives allows the informed <strong>R</strong> user
to choose the best tool for the task at hand.</p>
<p>One limitation of most of these algorithms is that they can be
difficult to use when there is a large number of decision variables.
Search methods can be inefficient with a massive number of parameters
because the search space is large, and they do not exploit information
about slope and curvature to speed up the time to convergence. Conjugate
gradient and quasi-Newton methods trace the curvature of the function by
using successive gradients to approximate the inverse Hessian. However,
if the algorithm stores the entire dense inverse Hessian, its use is
resource-intensive when the number of parameters is large. For example,
the Hessian for a 50,000 parameter model requires 20GB of RAM to store
it as a standard, dense base <strong>R</strong> matrix.</p>
<p>The <em>trustOptim</em> package provides a trust region algorithm
that is optimized for problems for which the Hessian is sparse. Sparse
Hessians occur when a large number of the cross-partial derivatives of
the objective function are zero. For example, suppose we want to find
the mode of a log posterior density for a Bayesian hierarchical model.
If we assume that individual-level parameter vectors
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>β</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\beta_i</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>β</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\beta_j</annotation></semantics></math>
are conditionally independent, the cross-partial derivatives between all
elements of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>β</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\beta_i</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>β</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\beta_j</annotation></semantics></math>
are zero. If the model includes a large number of heterogeneous units,
and a relatively small number of population-level parameters, the
proportion of non-zero entries in the Hessian will be small. Since we
know up front which elements of the Hessian are non-zero, we need to
compute, store, and operate on only those non-zero elements. By storing
sparse Hessians in a compressed format, and using a library of numerical
algorithms that are efficient for sparse matrices, we can run the
optimization algorithms faster, with a smaller memory footprint, than
algorithms that operate on dense Hessians.</p>
<p>The details of the trust region algorithm are included at the end of
this vignette. The vignette for the <em>sparseHessianFD</em> package
includes a more detailed discussion of Hessian sparsity patterns.</p>
</div>
<div class="section level2">
<h2 id="example-function">Example function<a class="anchor" aria-label="anchor" href="#example-function"></a>
</h2>
<p>Before going into the details of how to use the package, let’s
consider the following example of an objective function with a sparse
Hessian. Suppose we have a dataset of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>
households, each with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math>
opportunities to purchase a particular product. Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding="application/x-tex">y_i</annotation></semantics></math>
be the number of times household
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
purchases the product, out of the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math>
purchase opportunities. Furthermore, let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>p</mi><mi>i</mi></msub><annotation encoding="application/x-tex">p_i</annotation></semantics></math>
be the probability of purchase;
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>p</mi><mi>i</mi></msub><annotation encoding="application/x-tex">p_i</annotation></semantics></math>
is the same for all
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math>
opportunities, so we can treat
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding="application/x-tex">y_i</annotation></semantics></math>
as a binomial random variable. The purchase probability
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>p</mi><mi>i</mi></msub><annotation encoding="application/x-tex">p_i</annotation></semantics></math>
is heterogeneous, and depends on both
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
continuous covariates
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding="application/x-tex">x_i</annotation></semantics></math>,
and a heterogeneous coefficient vector
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>β</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\beta_i</annotation></semantics></math>,
such that
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>i</mi></msub><mo>=</mo><mfrac><mrow><mrow><mi mathvariant="normal">exp</mi><mo>⁡</mo></mrow><mo stretchy="false" form="prefix">(</mo><msubsup><mi>x</mi><mi>i</mi><mo>′</mo></msubsup><msub><mi>β</mi><mi>i</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><mrow><mn>1</mn><mo>+</mo><mrow><mi mathvariant="normal">exp</mi><mo>⁡</mo></mrow><mo stretchy="false" form="prefix">(</mo><msubsup><mi>x</mi><mi>i</mi><mo>′</mo></msubsup><msub><mi>β</mi><mi>i</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo>,</mo><mspace width="0.222em"></mspace><mi>i</mi><mo>=</mo><mn>1</mn><mi>.</mi><mi>.</mi><mi>.</mi><mi>N</mi></mrow><annotation encoding="application/x-tex">
  p_i=\frac{\exp(x_i'\beta_i)}{1+\exp(x_i'\beta_i)},~i=1 ... N
</annotation></semantics></math></p>
<p>The coefficients can be thought of as sensitivities to the
covariates, and they are distributed across the population of households
following a multivariate normal distribution with mean
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>μ</mi><annotation encoding="application/x-tex">\mu</annotation></semantics></math>
and covariance
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi mathvariant="normal">Σ</mi><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math>.
We assume that we know
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi mathvariant="normal">Σ</mi><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math>,
but we do not know
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>μ</mi><annotation encoding="application/x-tex">\mu</annotation></semantics></math>.
Instead, we place a multivariate normal prior on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>μ</mi><annotation encoding="application/x-tex">\mu</annotation></semantics></math>,
with mean
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>0</mn><annotation encoding="application/x-tex">0</annotation></semantics></math>
and covariance
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi mathvariant="normal">Ω</mi><mn>0</mn></msub><annotation encoding="application/x-tex">\Omega_0</annotation></semantics></math>.
Thus, each
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>β</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\beta_i</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>μ</mi><annotation encoding="application/x-tex">\mu</annotation></semantics></math>
are
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>−</mo></mrow><annotation encoding="application/x-tex">k-</annotation></semantics></math>dimensional
vectors, and the total number of unknown variables in the model is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><mi>N</mi><mo>+</mo><mn>1</mn><mo stretchy="false" form="postfix">)</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">(N+1)k</annotation></semantics></math>.</p>
<p>The log posterior density, ignoring any normalization constants, is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">log</mi><mo>⁡</mo></mrow><mi>π</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>β</mi><mrow><mn>1</mn><mo>:</mo><mi>N</mi></mrow></msub><mo>,</mo><mi>μ</mi><mo stretchy="false" form="prefix">|</mo><mi>Y</mi><mo>,</mo><mi>X</mi><mo>,</mo><msub><mi mathvariant="normal">Σ</mi><mn>0</mn></msub><mo>,</mo><msub><mi mathvariant="normal">Ω</mi><mn>0</mn></msub><mo stretchy="false" form="postfix">)</mo><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msubsup><mi>p</mi><mi>i</mi><msub><mi>y</mi><mi>i</mi></msub></msubsup><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>−</mo><msub><mi>p</mi><mi>i</mi></msub><msup><mo stretchy="false" form="postfix">)</mo><mrow><mi>T</mi><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub></mrow></msup><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>β</mi><mi>i</mi></msub><mo>−</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>′</mo></msup><msup><mi mathvariant="normal">Σ</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>β</mi><mi>i</mi></msub><mo>−</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><msup><mi>μ</mi><mo>′</mo></msup><msubsup><mi mathvariant="normal">Ω</mi><mn>0</mn><mrow><mi>−</mi><mn>1</mn></mrow></msubsup><mi>μ</mi></mrow><annotation encoding="application/x-tex">
  \log \pi(\beta_{1:N},\mu|Y, X, \Sigma_0,\Omega_0)=\sum_{i=1}^Np_i^{y_i}(1-p_i)^{T-y_i}
  -\frac{1}{2}\left(\beta_i-\mu\right)'\Sigma^{-1}\left(\beta_i-\mu\right)
-\frac{1}{2}\mu'\Omega_0^{-1}\mu
</annotation></semantics></math></p>
<p>Since the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>β</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\beta_i</annotation></semantics></math>
are drawn iid from a multivariate normal,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true"><mfrac><mrow><msup><mi>∂</mi><mn>2</mn></msup><mrow><mi mathvariant="normal">log</mi><mo>⁡</mo></mrow><mi>π</mi></mrow><mrow><mi>∂</mi><msub><mi>β</mi><mi>i</mi></msub><msub><mi>β</mi><mi>j</mi></msub></mrow></mfrac></mstyle><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\dfrac{\partial^2\log\pi }{\partial\beta_i\beta_j}=0</annotation></semantics></math>
for all
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo>≠</mo><mi>j</mi></mrow><annotation encoding="application/x-tex">i\neq
j</annotation></semantics></math>. We also know that all of the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>β</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\beta_i</annotation></semantics></math>
are correlated with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>μ</mi><annotation encoding="application/x-tex">\mu</annotation></semantics></math>.
The structure of the Hessian depends on how the variables are ordered
within the vector. One such ordering is to group all of the coefficients
for each unit together.</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mn>11</mn></msub><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi><mo>,</mo><msub><mi>β</mi><mrow><mn>1</mn><mi>k</mi></mrow></msub><mo>,</mo><msub><mi>β</mi><mn>21</mn></msub><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi><mo>,</mo><msub><mi>β</mi><mrow><mn>2</mn><mi>k</mi></mrow></msub><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi><mspace width="0.222em"></mspace><mo>,</mo><mspace width="0.222em"></mspace><mi>.</mi><mi>.</mi><mi>.</mi><mspace width="0.222em"></mspace><mo>,</mo><mspace width="0.222em"></mspace><msub><mi>β</mi><mrow><mi>N</mi><mn>1</mn></mrow></msub><mspace width="0.222em"></mspace><mo>,</mo><mspace width="0.222em"></mspace><mi>.</mi><mi>.</mi><mi>.</mi><mspace width="0.222em"></mspace><mo>,</mo><mspace width="0.222em"></mspace><msub><mi>β</mi><mrow><mi>N</mi><mi>k</mi></mrow></msub><mo>,</mo><msub><mi>μ</mi><mn>1</mn></msub><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi><mo>,</mo><msub><mi>μ</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">
\beta_{11},...,\beta_{1k},\beta_{21},...,\beta_{2k},...~,~...~,~\beta_{N1}~,~...~,~\beta_{Nk},\mu_1,...,\mu_k
</annotation></semantics></math></p>
<p>In this case, the Hessian has a “block-arrow” structure. For example,
if
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>6</mn></mrow><annotation encoding="application/x-tex">N=6</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">k=2</annotation></semantics></math>,
then there are 14 total variables, and the Hessian will have the
following pattern.</p>
<pre><code><span><span class="co"># 14 x 14 sparse Matrix of class "lgCMatrix"</span></span>
<span><span class="co">#                                  </span></span>
<span><span class="co">#  [1,] | | . . . . . . . . . . | |</span></span>
<span><span class="co">#  [2,] | | . . . . . . . . . . | |</span></span>
<span><span class="co">#  [3,] . . | | . . . . . . . . | |</span></span>
<span><span class="co">#  [4,] . . | | . . . . . . . . | |</span></span>
<span><span class="co">#  [5,] . . . . | | . . . . . . | |</span></span>
<span><span class="co">#  [6,] . . . . | | . . . . . . | |</span></span>
<span><span class="co">#  [7,] . . . . . . | | . . . . | |</span></span>
<span><span class="co">#  [8,] . . . . . . | | . . . . | |</span></span>
<span><span class="co">#  [9,] . . . . . . . . | | . . | |</span></span>
<span><span class="co"># [10,] . . . . . . . . | | . . | |</span></span>
<span><span class="co"># [11,] . . . . . . . . . . | | | |</span></span>
<span><span class="co"># [12,] . . . . . . . . . . | | | |</span></span>
<span><span class="co"># [13,] | | | | | | | | | | | | | |</span></span>
<span><span class="co"># [14,] | | | | | | | | | | | | | |</span></span></code></pre>
<p>There are 196 elements in this symmetric matrix, but only 76 are
non-zero, and only 45 values are unique. Although the reduction in RAM
from using a sparse matrix structure for the Hessian may be modest,
consider what would happen if
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>1000</mn></mrow><annotation encoding="application/x-tex">N=1000</annotation></semantics></math>
instead. In that case, there are 2002 variables in the problem, and more
than
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>4</mn><annotation encoding="application/x-tex">4</annotation></semantics></math>
million elements in the Hessian. However, only
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>12004</mn><annotation encoding="application/x-tex">12004</annotation></semantics></math>
of those elements are non-zero. If we work with only the lower triangle
of the Hessian we only need to work with only 7003 values.</p>
</div>
<div class="section level2">
<h2 id="using-the-package">Using the package<a class="anchor" aria-label="anchor" href="#using-the-package"></a>
</h2>
<p>The functions for computing the objective function, gradient and
Hessian for this example are in the R/binary.R file. The package also
includes a sample dataset with simulated data from the binary choice
example. This dataset can be access with the <code>data(binary)</code>
call.</p>
<p>To start, we load the data, set some dimension parameters, set prior
values for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi mathvariant="normal">Σ</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\Sigma^{-1}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi mathvariant="normal">Ω</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\Omega^{-1}</annotation></semantics></math>,
and simulate a vector of variables at which to evaluate the
function.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="va">binary</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/str.html" class="external-link">str</a></span><span class="op">(</span><span class="va">binary</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co"># List of 3</span></span>
<span><span class="co">#  $ Y: int [1:200] 16 1 30 70 51 52 0 27 59 15 ...</span></span>
<span><span class="co">#  $ X: num [1:2, 1:200] 1.5587 0.0705 0.1293 1.7151 0.4609 ...</span></span>
<span><span class="co">#  $ T: num 100</span></span></code></pre>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">binary</span><span class="op">$</span><span class="va">Y</span><span class="op">)</span></span>
<span><span class="va">k</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html" class="external-link">NROW</a></span><span class="op">(</span><span class="va">binary</span><span class="op">$</span><span class="va">X</span><span class="op">)</span></span>
<span><span class="va">nvars</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/integer.html" class="external-link">as.integer</a></span><span class="op">(</span><span class="va">N</span><span class="op">*</span><span class="va">k</span> <span class="op">+</span> <span class="va">k</span><span class="op">)</span></span>
<span><span class="va">start</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="va">nvars</span><span class="op">)</span> <span class="co">## random starting values</span></span>
<span><span class="va">priors</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>inv.Sigma <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/rWishart.html" class="external-link">rWishart</a></span><span class="op">(</span><span class="fl">1</span>,<span class="va">k</span><span class="op">+</span><span class="fl">5</span>,<span class="fu"><a href="https://rdrr.io/r/base/diag.html" class="external-link">diag</a></span><span class="op">(</span><span class="va">k</span><span class="op">)</span><span class="op">)</span><span class="op">[</span>,,<span class="fl">1</span><span class="op">]</span>,</span>
<span>               inv.Omega <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/diag.html" class="external-link">diag</a></span><span class="op">(</span><span class="va">k</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>This dataset represents the simulated choices for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>200</mn></mrow><annotation encoding="application/x-tex">N= 200</annotation></semantics></math>
customers over
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo>=</mo><mn>100</mn></mrow><annotation encoding="application/x-tex">T= 100</annotation></semantics></math>
purchase opportunties, where the probability of purchase is influenced
by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">k= 2</annotation></semantics></math>
covariates.</p>
<p>The objective function for the binary choice example is
<code>binary.f</code>, the gradient function is
<code>binary.grad</code>, and the Hessian function is
<code>binary.hess</code>. The first argument to both is the variable
vector, and the argument lists must be the same for both. For this
example, we need to provide the data list “binary”
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math>)
and the prior parameter list
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi mathvariant="normal">Σ</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\Sigma^{-1}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi mathvariant="normal">Ω</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\Omega^{-1}</annotation></semantics></math>).
The <code>binary.hess</code> function returns the Hessian as a
<code>dgCMatrix</code> object, which is a compressed sparse matrix class
defined in the Matrix package.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">opt</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/trust.optim.html">trust.optim</a></span><span class="op">(</span><span class="va">start</span>, fn<span class="op">=</span><span class="va">binary.f</span>,</span>
<span>                   gr <span class="op">=</span> <span class="va">binary.grad</span>,</span>
<span>                   hs <span class="op">=</span> <span class="va">binary.hess</span>,</span>
<span>                   method <span class="op">=</span> <span class="st">"Sparse"</span>,</span>
<span>                   control <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>                       start.trust.radius<span class="op">=</span><span class="fl">5</span>,</span>
<span>                       stop.trust.radius <span class="op">=</span> <span class="fl">1e-7</span>,</span>
<span>                       prec<span class="op">=</span><span class="fl">1e-7</span>,</span>
<span>                       report.precision<span class="op">=</span><span class="fl">1L</span>,</span>
<span>                       maxit<span class="op">=</span><span class="fl">500L</span>,</span>
<span>                       preconditioner<span class="op">=</span><span class="fl">1L</span>,</span>
<span>                       function.scale.factor<span class="op">=</span><span class="op">-</span><span class="fl">1</span></span>
<span>                   <span class="op">)</span>,</span>
<span>                   data<span class="op">=</span><span class="va">binary</span>, priors<span class="op">=</span><span class="va">priors</span></span>
<span>                   <span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co"># Beginning optimization</span></span>
<span><span class="co"># </span></span>
<span><span class="co"># iter       f     nrm_gr                     status</span></span>
<span><span class="co">#   1   15977.1    769.5     Continuing - TR expand</span></span>
<span><span class="co">#   2   12359.7    278.2                 Continuing</span></span>
<span><span class="co">#   3   12359.7    278.2   Continuing - TR contract</span></span>
<span><span class="co">#   4   12059.7    261.9                 Continuing</span></span>
<span><span class="co">#   5   12059.7    261.9   Continuing - TR contract</span></span>
<span><span class="co">#   6   11670.0    310.8                 Continuing</span></span>
<span><span class="co">#   7   11670.0    310.8   Continuing - TR contract</span></span>
<span><span class="co">#   8   11440.0     67.5     Continuing - TR expand</span></span>
<span><span class="co">#   9   11304.5      7.2     Continuing - TR expand</span></span>
<span><span class="co">#  10   11303.8      1.0                 Continuing</span></span>
<span><span class="co">#  11   11303.8      0.0                 Continuing</span></span>
<span><span class="co">#  12   11303.8      0.0                 Continuing</span></span>
<span><span class="co">#  13   11303.8      0.0                 Continuing</span></span>
<span><span class="co"># </span></span>
<span><span class="co"># Iteration has terminated</span></span>
<span><span class="co">#  13   11303.8      0.0                    Success</span></span></code></pre>
<div class="section level3">
<h3 id="control-parameters">Control parameters<a class="anchor" aria-label="anchor" href="#control-parameters"></a>
</h3>
<p>The <code>control</code> argument takes a list of options, all of
which are described in the package manual. Most of these arguments are
related to the internal workings of the trust region algorithm, such as
how close a step needs to be to the border of the trust region before
the region expands. However, there are a few arguments that deserve some
special attention.</p>
<div class="section level4">
<h4 id="scaling-the-objective-function">Scaling the objective function<a class="anchor" aria-label="anchor" href="#scaling-the-objective-function"></a>
</h4>
<p>The algorithms in the package <em>minimize</em> the objective
function by default. When the <code>function.scale.factor</code> option
is specified, the objective function, gradient and Hessian are all
multiplied by that value throughout the optimization procedure. If
<code>function.scale.factor=-1</code>, then then
<code>trust.optim</code> will maximize the objective function.</p>
</div>
<div class="section level4">
<h4 id="stopping-criteria">Stopping criteria<a class="anchor" aria-label="anchor" href="#stopping-criteria"></a>
</h4>
<p>The <code>trust.optim</code> function will stop when the Euclidean
norm of the gradient is less that <code>sqrt(nvars) * prec</code>, where
<code>nvars</code> is the length of the parameter vector, and
<code>prec</code> is specified in the control list (the default is
<code>sqrt(.Machine\$double.eps)</code>, which is the square root of the
computer’s floating point precision. However, sometimes the algorithm
cannot get the gradient to be that flat. When that occurs, the trust
region will shrink, until its radius is less than the value of the
<code>stop.trust.radius</code> parameter. The algorithm will then stop
with the message “Radius of trust region is less than
<code>stop.trust.radius</code>.” This event is not necessarily a problem
if the norm of the gradient is still small enough that the gradient is
flat for all practical purposes. For example, suppose we set
<code>prec</code> to be
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mn>10</mn><mrow><mi>−</mi><mn>7</mn></mrow></msup><annotation encoding="application/x-tex">10^{-7}</annotation></semantics></math>
and that, for numerical reasons, the norm of the gradient simply cannot
get below
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mn>10</mn><mrow><mi>−</mi><mn>6</mn></mrow></msup><annotation encoding="application/x-tex">10^{-6}</annotation></semantics></math>.
If the norm of the gradient were the only stopping criterion, the
algorithm would continue to run, even though it is probably close enough
to the local optimum. With this alternative stopping criterion, the
algorithm will also stop when it is clear that the algorithm can no
longer take a step that leads to an improvement in the objective
function.</p>
<p>There is, of course, a third stopping criterion. The
<code>maxit</code> argument is the maximum number of iterations the
algorithm should run before stopping. However, keep in mind that if the
algorithm stops at <code>maxit</code>, it is almost certainly not at a
local optimum.</p>
<blockquote>
<blockquote>
<p>Note that many other nonlinear optimizers, including
<code>optim</code>, do not use the norm of the gradient as a stopping
criterion. Instead, they stop when the absolute or relative changes in
the objective function are less than some tolerance value. This often
causes those optimizers to stop prematurely, when the estimates of the
gradient and/or Hessian are not precise, or if there are some regions of
the domain where the objective function is nearly flat. In theory, this
should never happen, but in reality, it happens <em>all the time</em>.
For an unconstrained optimization problem, there is no reason why the
norm of the gradient should not be zero (within numerical precision)
before the algorithm stops.</p>
</blockquote>
</blockquote>
</div>
</div>
<div class="section level3">
<h3 id="preconditioners">Preconditioners<a class="anchor" aria-label="anchor" href="#preconditioners"></a>
</h3>
<p>Currently, the package offers two preconditioners: an identity
preconditioner (no preconditioning), and an inexact modified Cholesky
preconditioner, as in Algorithm 7.3 of <span class="citation">Nocedal
and Wright (2006)</span>. The identity and diagonal preconditioners are
available for all of the methods. For the <em>Sparse</em> method, the
modified Cholesky preconditioner will use a positive definite matrix
that is close to the potentially indefinite Hessian
(<code>trust.optim</code> does <em>not</em> require that the objective
function be positive definite). For <em>BFGS</em>, the modified Cholesky
preconditioner is available because <em>BFGS</em> updates are always
positive definite. If the user selects a modified Cholesky
preconditioner for <em>SR1</em>, the algorithm will use the identity
preconditioner instead.</p>
<p>There is no general rule for selecting preconditioners. There will be
a tradeoff between the number of iterations needs to solve the problem
and the time it takes to compute any particular preconditioner. In some
cases, the identity preconditioner may even solve the problem in fewer
iterations than a modified Cholesky preconditioner.</p>
</div>
<div class="section level3">
<h3 id="result-object">Result object<a class="anchor" aria-label="anchor" href="#result-object"></a>
</h3>
<p>The call ot <code>trust.optim</code> returns a list of values.</p>
<ul>
<li>
<strong>fval</strong>: the value of the objective function at the
optimum</li>
<li>
<strong>solution</strong>: the optimum</li>
<li>
<strong>gradient</strong>: the gradient of the objective function at
the optimum (all elements should be very close to zero)</li>
<li>
<strong>hessian</strong>: the Hessian of the objective function at
the optimum, as an object of class <em>dsCMatrix</em>.</li>
<li>
<strong>iterations</strong>: number of iterations</li>
<li>
<strong>status</strong>: A status message (should be “Success”), or
possibly a note that the trust region radius is less than
<code>stop.trust.region</code>.</li>
<li>
<strong>trust.radius</strong>: trust region radius when the
algorithm stopped.</li>
<li>
<strong>nnz</strong>: number of nonzero elements in the lower
triangle of the Hessian</li>
<li>
<strong>method</strong>: the optimization method that was used
(Sparse, SR1 or BFGS).</li>
<li>
<strong>nnz</strong>: for the Sparse method only, the number of
nonzero elements in the Hessian.</li>
</ul>
<p>See the package manual for more details.</p>
</div>
</div>
<div class="section level2">
<h2 id="algorithmic-details">Algorithmic details<a class="anchor" aria-label="anchor" href="#algorithmic-details"></a>
</h2>
<p>Consider
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math>,
an objective function over a
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math>-dimensional
vector that we want to minimize. Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math>
be the gradient, and let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>
be the Hessian. The goal is to find a local minimum of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math>,
with no constraints on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>,
within some window of numerical precision (i.e., where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mi>g</mi><msub><mo stretchy="false" form="postfix">∥</mo><mn>2</mn></msub><mi>/</mi><msqrt><mi>p</mi></msqrt><mo>&lt;</mo><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\|g\|_2 / \sqrt{p}&lt;\epsilon</annotation></semantics></math>
for small
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\epsilon&gt;0</annotation></semantics></math>).
We will assume that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>
is positive definite at the local optimum, but not necessarily at other
values of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>.
Iterations are indexed by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>.</p>
<div class="section level3">
<h3 id="trust-region-methods-for-nonlinear-optimization">Trust region methods for nonlinear optimization<a class="anchor" aria-label="anchor" href="#trust-region-methods-for-nonlinear-optimization"></a>
</h3>
<p>The details of trust region methods are described in both <span class="citation">Nocedal and Wright (2006)</span> and <span class="citation">Conn et al. (2000)</span>, and the following exposition
borrows heavily from both sources. At each iteration of a trust region
algorithm, we construct a quadratic approximation to the objective
function at
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mi>t</mi></msub><annotation encoding="application/x-tex">x_t</annotation></semantics></math>,
and minimize that approximation, subject to a constraint that the
solution falls within a trust region with radius
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>d</mi><mi>t</mi></msub><annotation encoding="application/x-tex">d_t</annotation></semantics></math>.
More formally, each iteration of the trust region algorithm involves
solving the “trust region subproblem,” or TRS.</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"><munder><mi mathvariant="normal">min</mi><mrow><mi>s</mi><mo>∈</mo><msup><mi>R</mi><mi>p</mi></msup></mrow></munder><msup><mi>f</mi><mo>*</mo></msup><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><mi>f</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>+</mo><msubsup><mi>g</mi><mi>t</mi><mi>⊤</mi></msubsup><mi>s</mi><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><msup><mi>s</mi><mi>⊤</mi></msup><msub><mi>B</mi><mi>t</mi></msub><mi>s</mi><mspace width="2.0em"></mspace><mrow><mtext mathvariant="normal">s.t. </mtext><mspace width="0.333em"></mspace></mrow><mo stretchy="false" form="postfix">∥</mo><mi>s</mi><msub><mo stretchy="false" form="postfix">∥</mo><mi>M</mi></msub><mo>≤</mo><msub><mi>d</mi><mi>t</mi></msub></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"><msub><mi>s</mi><mi>t</mi></msub></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><mrow><mi mathvariant="normal">arg</mi><mo>⁡</mo></mrow><munder><mi mathvariant="normal">min</mi><mrow><mi>s</mi><mo>∈</mo><msup><mi>R</mi><mi>p</mi></msup></mrow></munder><msup><mi>f</mi><mo>*</mo></msup><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo><mspace width="2.0em"></mspace><mrow><mtext mathvariant="normal">s.t. </mtext><mspace width="0.333em"></mspace></mrow><mo stretchy="false" form="postfix">∥</mo><mi>s</mi><msub><mo stretchy="false" form="postfix">∥</mo><mi>M</mi></msub><mo>≤</mo><msub><mi>d</mi><mi>t</mi></msub></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{align}
\tag{TRS}\label{eq:TRS}
\min_{s\in R^p} f^*(s)&amp; = f(x_t) + g_t^\top s + \frac{1}{2}s^\top B_ts\qquad\text{s.t. }\|s\|_M\leq d_t\\
s_t&amp;=\arg\min_{s\in R^p} f^*(s) \qquad\text{s.t. }\|s\|_M\leq d_t
\end{align}
</annotation></semantics></math></p>
<p>The norm
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mo>⋅</mo><msub><mo stretchy="false" form="postfix">∥</mo><mi>M</mi></msub></mrow><annotation encoding="application/x-tex">\|\cdot\|_M</annotation></semantics></math>
is a Mahalanobis norm with respect to some positive definite matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math>.</p>
<p>Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>s</mi><mi>t</mi></msub><annotation encoding="application/x-tex">s_t</annotation></semantics></math>
be the solution to the for iteration
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>,
and consider the ratio
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>ρ</mi><mi>t</mi></msub><mo>=</mo><mfrac><mrow><mi>f</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>−</mo><mi>f</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>+</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><mrow><msup><mi>f</mi><mo>*</mo></msup><mo stretchy="false" form="prefix">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>−</mo><msup><mi>f</mi><mo>*</mo></msup><mo stretchy="false" form="prefix">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>+</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow></mfrac></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
  \label{eq:2}
  \rho_t=\frac{f(x_t)-f(x_t+s_t)}{f^*(x_t)-f^*(x_t+s_t)}
\end{align}</annotation></semantics></math> This ratio is the
improvement in the objective function that we would get from a move from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mi>t</mi></msub><annotation encoding="application/x-tex">x_t</annotation></semantics></math>
to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">x_{t+1}</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>x</mi><mi>t</mi></msub><mo>+</mo><msub><mi>s</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_{t+1}=x_t+s_t</annotation></semantics></math>,
relative to the improvement that is predicted by the quadratic
approximation. Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>η</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\eta_1</annotation></semantics></math>
be the minimum value of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>ρ</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\rho_t</annotation></semantics></math>
for which we deem it “worthwhile” to move from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mi>t</mi></msub><annotation encoding="application/x-tex">x_t</annotation></semantics></math>
to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">x_{t+1}</annotation></semantics></math>,
and let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>η</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\eta_2</annotation></semantics></math>
be the maximum
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>ρ</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\rho_t</annotation></semantics></math>
that would trigger a shrinkage in the trust region. If
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ρ</mi><mi>t</mi></msub><mo>&lt;</mo><msub><mi>η</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\rho_t &lt; \eta_2</annotation></semantics></math>,
or if
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>+</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">f(x_t+s_t)</annotation></semantics></math>
is not finite, we shrink the trust region by reducing
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>d</mi><mi>t</mi></msub><annotation encoding="application/x-tex">d_t</annotation></semantics></math>
by some predetermined factor, and compute a new
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>s</mi><mi>t</mi></msub><annotation encoding="application/x-tex">s_t</annotation></semantics></math>
by solving the again. If
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ρ</mi><mi>t</mi></msub><mo>&gt;</mo><msub><mi>η</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\rho_t&gt;\eta_1</annotation></semantics></math>,
we move to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>x</mi><mi>t</mi></msub><mo>+</mo><msub><mi>s</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_{t+1}=x_t+s_t</annotation></semantics></math>.
Also, if we do accept the move, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>s</mi><mi>t</mi></msub><annotation encoding="application/x-tex">s_t</annotation></semantics></math>
is on the border of the trust region, we expand the trust region by
increasing
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>d</mi><mi>t</mi></msub><annotation encoding="application/x-tex">d_t</annotation></semantics></math>,
again by some predetermined factor. The idea is to not move to a new
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>
if
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">f(x_{t+1})</annotation></semantics></math>
would be worse than
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">f(x_t)</annotation></semantics></math>.
By expanding the trust region, we can propose larger jumps, and
potentially reach the optimum more quickly. We want to propose only
moves that are among those that we “trust” to give reasonable values of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math>.
If it turns out that a move leads to a large improvement in the
objective function, and that the proposed move was constrained by the
radius of the trust region, we want to expand the trust region so we can
take larger steps. If the proposed move is bad, we should then reduce
the size of the region we trust, and try to find another step that is
closer to the current iterate. Of course, there is no reason that the
trust region needs to change after a particular iteration, especially if
the solution to the TRS is at an internal point.</p>
<p>There are a number of different ways to solve the TRS; <span class="citation">Conn et al. (2000)</span> is authoritative and
encyclopedic in this area. The <em>trustOptim</em> package uses the
method described in <span class="citation">Steihaug (1983)</span>. The
Steihaug algorithm is, essentially, a conjugate gradient solver for a
constrained quadratic program. If
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>B</mi><mi>t</mi></msub><annotation encoding="application/x-tex">B_t</annotation></semantics></math>
is positive definite, the Steihaug solution to the will be exact, up to
some level of numerical precision. However, if
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>B</mi><mi>t</mi></msub><annotation encoding="application/x-tex">B_t</annotation></semantics></math>
is indefinite, the algorithm could try to move in a direction of
negative curvature. If the algorithm happens to stumble on such a
direction, it goes back to the last direction that it moved, runs in
that direction to the border of the trust region, and returns that point
of intersection with the trust region border as the “solution” to the .
This solution is not necessarily the true minimizer of the , but it
still might provide sufficient improvement in the objective function
such that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ρ</mi><mi>t</mi></msub><mo>&gt;</mo><msub><mi>η</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\rho_t&gt;\eta_1</annotation></semantics></math>.
If not, we shrink the trust region and try again. As an alternative to
the Steihaug algorithm for solving the , <span class="citation">Conn et
al. (2000)</span> suggest using the Lanczos algorithm. The Lanczos
approach may be more likely to find a better solution to the TRS when
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>B</mi><mi>t</mi></msub><annotation encoding="application/x-tex">B_t</annotation></semantics></math>
is indefinite, but at some additional computational cost. We include
only the Steihaug algorithm for now, because it still seems to work
well, especially for sparse problems.</p>
<p>As with other conjugate gradient methods, one way to speed up the
Steihaug algorithm is to rescale the trust region subproblem with a
preconditioner
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math>.
Note that the constraint in is expressed as an
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math>-norm,
rather than an Euclidean norm. The positive definite matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math>
should be close enough to the Hessian that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>M</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><msub><mi>B</mi><mi>t</mi></msub><mo>≈</mo><mi>I</mi></mrow><annotation encoding="application/x-tex">M^{-1}B_t\approx I</annotation></semantics></math>,
but still cheap enough to compute that the cost of using the
preconditioner does not exceed the benefits. Of course, the ideal
preconditioner would be
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>B</mi><mi>t</mi></msub><annotation encoding="application/x-tex">B_t</annotation></semantics></math>
itself, but
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>B</mi><mi>t</mi></msub><annotation encoding="application/x-tex">B_t</annotation></semantics></math>
is not necessarily positive definite, and we may not be able to estimate
it fast enough for preconditioning to be worthwhile. In this case, one
could use a modified Cholesky decomposition, as described in <span class="citation">Nocedal and Wright (2006)</span>.</p>
</div>
</div>
<div class="section level2">
<h2 class="unnumbered" id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-R_trustOptim" class="csl-entry">
Braun, Michael. 2014. <span>“<span class="nocase">t</span>rust<span>O</span>ptim: An <span>R</span> Package
for Trust Region Optimization with Sparse Hessians.”</span> <em>Journal
of Statistical Software</em> 60 (4): 1–16. <a href="https://www.jstatsoft.org/v60/i04/" class="external-link">https://www.jstatsoft.org/v60/i04/</a>.
</div>
<div id="ref-ConnGould2000" class="csl-entry">
Conn, Andrew R, Nicholas I M Gould, and Philippe L Toint. 2000.
<em>Trust-Region Methods</em>. SIAM-MPS.
</div>
<div id="ref-NocedalWright2006" class="csl-entry">
Nocedal, Jorge, and Stephen J Wright. 2006. <em>Numerical
Optimization</em>. Second edition. Springer-Verlag.
</div>
<div id="ref-Steihaug1983" class="csl-entry">
Steihaug, Trond. 1983. <span>“The Conjugate Gradient Method and Trust
Regions in Large Scale Optimization.”</span> <em><span>SIAM</span>
Journal on Numerical Analysis</em> 20 (3): 626–37. <a href="https://doi.org/10.1137/0720042" class="external-link">https://doi.org/10.1137/0720042</a>.
</div>
<div id="ref-R_OptimTaskView" class="csl-entry">
Theussel, Stefan. 2013. <span>“<span>CRAN</span> Task View: Optimization
and Mathematical Programming.”</span> <a href="https://cran.r-project.org/view=Optimization" class="external-link">https://cran.r-project.org/view=Optimization</a>.
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

      </div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by <a href="https://braunm.github.io" class="external-link">Michael Braun</a>.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.2.0.</p>
</div>

      </footer>
</div>






  </body>
</html>
